# Sentiment Analysis

Read the [Russian version](./README_RU.md) of the document üá∑üá∫

## Problem description

Make a classifier of the emotional tonality of short text snippets (for example, tweets). The algorithm should be able to classify messages into two classes: messages with positive emotionality and messages with negative emotionality.

## Tools and libraries

* Python 3;
* Pandas for working with data;
* Scikit-Learn for tokenization, cross-validation and machine learning algorithms;
* PyMorphy2 for lemmatization of Russian words.

## Training set

Creating a training sample is a complex and time-consuming activity. It takes a lot of time and the help of a lot of people (assessors) to create and classify even a small training set.

I will use the [ready set](http://study.mokoron.com/) consisting of approximately 225 thousand classified tweets (positive or negative emotionality).

## Working with training set

The training set contains not only the texts of tweets and class tags, but also a large amount of additional information (publication dates, usernames, number of retweets, etc). In the context of this task, this information is not needed, so we leave only texts and labels in the dataset.

Next, process the tweet lines:

* Delete all English words;
* Remove all punctuation (it does not contain any semantic);
* Remove all usernames, tags about retweet (RT) and links;
* Transform all words to lowercase and use the PyMorphy2 module to lemmatize words.

Save the processed dataset to a new cleaned_data file.csv. We will continue to work with it.

## –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

–ó–∞–¥–∞—á–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ —Ç–æ–º, —á—Ç–æ–±—ã –≤—ã–±—Ä–∞—Ç—å —Å–æ—á–µ—Ç–∞–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞, –º–µ—Ç–æ–¥–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏, —Å—Ö–µ–º—ã n-–≥—Ä–∞–º–º –∏ –ø—Ä–æ—á–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —á—Ç–æ–±—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.

–Ø –≤—ã–±—Ä–∞–ª –¥–≤–µ –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ (–ø–æ –º–æ–µ–º—É –º–Ω–µ–Ω–∏—é) –º–æ–¥–µ–ª–∏: –Ω–∞–∏–≤–Ω—ã–π –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏ –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä (–º–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞). –ò–Ω–æ–≥–¥–∞ –¥–ª—è –ø–æ–¥–æ–±–Ω–æ–≥–æ —Ä–æ–¥–∞ –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É—é—Ç SVM, –Ω–æ –æ–Ω –∫—Ä–∞–π–Ω–µ –º–µ–¥–ª–µ–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –æ–±—ä–µ–∫—Ç–æ–≤ –∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –õ–æ–≥–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –≤–æ–≤—Å–µ –Ω–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è, —Ç–∞–∫ –∫–∞–∫ —Å–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –∫ –¥–∞–Ω–Ω–æ–π –∑–∞–¥–∞—á–µ.

–ë–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –æ—Å–æ–±–æ –Ω–µ –Ω—É–∂–¥–∞–µ—Ç—Å—è –≤ –ø–æ–¥–±–æ—Ä–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –∞ –≤–æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ª–∏–Ω–µ–π–Ω–æ–π –º–æ–¥–µ–ª–∏ —è –±—É–¥—É –ø–æ–¥–±–∏—Ä–∞—Ç—å –ø–æ —Å–µ—Ç–∫–µ.

–¢–∞–∫ –∂–µ —è –±—É–¥—É –ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–≤–∞ –º–µ—Ç–æ–¥–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: Count Vectorizer –∏ TF-IDF Vectorizer. 

–ü–æ—Å–ª–µ–¥–Ω–µ–µ, —á—Ç–æ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ –ø–æ–¥–æ–±—Ä–∞—Ç—å ‚Äì —Å—Ö–µ–º—É n-–≥—Ä–∞–º–º. –û–±—ã—á–Ω–æ –¥–ª—è —Ç–∞–∫–∏—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å—Ö–µ–º—ã —Å —É–Ω–∏–≥—Ä–∞–º–º–Ω—ã–º–∏, –±–∏–≥—Ä–∞–º–Ω—ã–º–∏ –∏–ª–∏ —Ç—Ä–∏–≥—Ä–∞–º–º–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏, –∞ —Ç–∞–∫–∂–µ –∏—Ö —Å–æ–≤–º–µ—Å—Ç–Ω—ã–µ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏.

## –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å

–ü–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –ª—É—á—à–µ–π –æ–∫–∞–∑–∞–ª–∞—Å—å —Å–ª–µ–¥—É—é—â–∞—è –º–æ–¥–µ–ª—å:

* –°—Ö–µ–º–∞ n-–≥—Ä–∞–º–º: **(1, 3)** (—É–Ω–∏–≥—Ä–∞–º–º—ã + –±–∏–≥—Ä–∞–º–º—ã + —Ç—Ä–∏–≥—Ä–∞–º–º—ã);
* Vectorizer: **TF-IDF**;
* –¢–∏–ø –º–æ–¥–µ–ª–∏: **–ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å**;
* –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏: **penalty ‚Äì l2, alpha ‚Äì 0.000001, loss ‚Äì log**.

–ï—ë —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ–∫–∞–∑–∞–ª—Å—è ‚âà 0.75.
